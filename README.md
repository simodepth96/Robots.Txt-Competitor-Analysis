# Robots.Txt Competitor Analysis

I've always found it tricky and nerve-racking to run a competitor analysis considering the robots.txt files across several websites.

But time has served me well, and for a recent competitive analysis I decided to go down that route once and for all.

In this repository, you will find access to the raw python exported from Google Colab: 
The script breaks down a sample of 4 competitors from the publishing industry and returns:
- Distribution of Blocked User-Agents 
<img width="1307" height="506" alt="image" src="https://github.com/user-attachments/assets/7a6bf2e4-b062-4394-88c7-782addc16a86" />
- Distribution of Disallowed Directives (After Rule-based clustering)
<img width="1321" height="497" alt="image" src="https://github.com/user-attachments/assets/2f3461fa-a3c5-460a-92a3-293216b45b22" />
 



